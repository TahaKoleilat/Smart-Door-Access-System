# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/123hbkGRuZ5ukX2Rl9jZqPA5qEYvZpAWv
"""

!pip install opendatasets

import pandas as pd
import numpy as np
import os
from numpy import asarray
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns

import opendatasets as od

from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Conv2D,Dense, MaxPooling2D,Flatten,Dropout,Activation, BatchNormalization
from tensorflow.keras.optimizers import Adam,RMSprop,SGD
from keras import regularizers
import tensorflow as tf
from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau
import datetime
from tensorflow.keras.utils import plot_model

od.download("https://www.kaggle.com/datasets/sumansid/facemask-dataset",data_dir="dataset1")
od.download("https://www.kaggle.com/datasets/ahemateja19bec1025/facemask-dataset",data_dir="dataset2")
od.download("https://www.kaggle.com/datasets/vinaykudari/facemask",data_dir="dataset3")
od.download("https://www.kaggle.com/datasets/pranavsingaraju/facemask-detection-dataset-20000-images",data_dir="dataset4")
od.download("https://www.kaggle.com/datasets/ashiqurrahmantanzil/facemask-dataset",data_dir="dataset5")

data=[]
labels=[]
CATEGORIES = ["Mask/Mask", "No Mask/No Mask"]
directory = '/content/dataset1/facemask-dataset'
for category in CATEGORIES:
    path = os.path.join(directory, category)
    for img in os.listdir(path):
        img_path = os.path.join(path, img)
      
        # load the image
        image = Image.open(img_path).convert('RGB')

        image=image.resize((128,128))
        # convert image to numpy array
        image_data = asarray(image)
        data.append(image_data)

        # perform one-hot encoding on the labels
        if category == "Mask/Mask":
            labels.append(0)
        else:
            labels.append(1)

print(data[1].shape)

CATEGORIES = ["1", "0"]
directory = '/content/dataset2/facemask-dataset/dataset/dataset'
for category in CATEGORIES:
    path = os.path.join(directory, category)
    for img in os.listdir(path):
        img_path = os.path.join(path, img)
      
        # load the image
        image = Image.open(img_path).convert('RGB')

        image=image.resize((128,128))
        # convert image to numpy array
        image_data = asarray(image)
        data.append(image_data)

        # perform one-hot encoding on the labels
        if category == "0":
            labels.append(1)
        else:
            labels.append(0)

print(labels)

CATEGORIES = ["with_mask", "without_mask"]
directory = '/content/dataset3/facemask/train'
for category in CATEGORIES:
    path = os.path.join(directory, category)
    for img in os.listdir(path):
        img_path = os.path.join(path, img)
      
        # load the image
        image = Image.open(img_path).convert('RGB')

        image=image.resize((128,128))

        # convert image to numpy array
        image_data = asarray(image)
        data.append(image_data)

        # perform one-hot encoding on the labels
        if category == "with_mask":
            labels.append(0)
        else:
            labels.append(1)

CATEGORIES = ["new_with_mask", "new_without_mask"]
directory = '/content/dataset4/facemask-detection-dataset-20000-images'
for category in CATEGORIES:
    path = os.path.join(directory, category)
    for img in os.listdir(path):
        img_path = os.path.join(path, img)
      
        # load the image
        image = Image.open(img_path).convert('RGB')

        image=image.resize((128,128))

        # convert image to numpy array
        image_data = asarray(image)
        data.append(image_data)

        # perform one-hot encoding on the labels
        if category == "new_with_mask":
            labels.append(0)
        else:
            labels.append(1)

print(data[0].shape)

from sklearn.model_selection import train_test_split
print(len(labels))
data = np.array(data)
labels = np.array(labels)

(trainX, testX, trainY, testY) = train_test_split(data, labels,
                                                  test_size=0.20, stratify=labels, random_state=42)

print(len(data))

import numpy as np 
import pandas as pd


import seaborn as sn
from tensorflow import keras
from sklearn.metrics import confusion_matrix, classification_report
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt

def build_model():
 model = tf.keras.models.Sequential([ 

    tf.keras.layers.Conv2D(100, (3, 3), activation='relu', 

                           input_shape=(128, 128, 3)), 

    tf.keras.layers.MaxPooling2D(2, 2), 

   


    tf.keras.layers.MaxPooling2D(2, 2), 

   

    tf.keras.layers.Flatten(), 


    tf.keras.layers.Dense(1, activation='sigmoid') ]) 
 
 return model

model=build_model()
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
model.summary()

model.fit(trainX, trainY , epochs = 5,validation_split=0.2)

loss, accuracy  = model.evaluate(testX,testY, verbose=False)

print(f'Validation loss: {loss:.3}')
print(f'Validation accuracy: {accuracy:.3}')

from keras.models import load_model

model.save('model_weights.h5')

model_json = model.to_json()
with open("model.json","w") as json_file:
    json_file.write(model_json)